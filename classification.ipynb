{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMi1dGcxBYj1eqA5OLqrgod"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1y_-miW8qZcU"},"source":["# Theory"]},{"cell_type":"markdown","metadata":{"id":"cIDkPwuAslp8"},"source":["In this chapter, we are interested in predicting qualitative responses $G(x)$, given a vector of inputs $x=(x_1,x_2,\\ldots,x_p)$. The predictor $G(x)$ takes values in a discrete set $C$, $G(x)\\in C$, comprised of $K$ classes, labelled as $1, 2, \\ldots, K$. Estimation theory ensures optimal probability if the posterior probability $Pr(G|x)$ is known. Then, we could classify observations to the class with largest probability, following the Bayes classifier, which leads to the lowest error rate.\n"]},{"cell_type":"markdown","metadata":{"id":"qquQH33LxpMb"},"source":["## Linear regression"]},{"cell_type":"markdown","metadata":{"id":"cmOJpH0Cx6HD"},"source":["One could use linear regression to model a class indicator function and classify to the largest fit\n","\n","$$\n","\\delta_k(x)=\\beta_{k0}+\\beta_k^Tx,\n","$$\n","\n","where $\\beta_k=(\\beta_{k1},\\ldots,\\beta_{kp})$. Linear $Pr(G=k|X=x)$ leads to linear decision boundaries $\\{x:(\\beta_{k0}-\\beta_{l0})+(\\beta_k-\\beta_l)^Tx=0\\}$. For a binary response, linear regression is equivalent to linear discriminant analysis, which assumes response densities for each class to be Gaussian. The problem with linear regression is that the output of the regression can be negative or larger than one. "]},{"cell_type":"markdown","metadata":{"id":"G6ws8HQczDcD"},"source":["## Logistic regression"]},{"cell_type":"markdown","metadata":{"id":"2uG4uXg3zINc"},"source":["Logistic regression uses the *logit* transformation, $logit(p)=\\log[p/(1-p)]$, to model a linear discriminant function $\\delta_k(x)$. For a binary problem, with classes $G=\\{1,2\\}$, the posterior probabilities are modelled as follows: \n","\n","\\begin{align}\n","p(x)&=Pr(G=1|X=x)=\\frac{\\exp{(\\beta_0+\\beta_1^Tx)}}{1+\\exp{(\\beta_0+\\beta^Tx)}},\\\\\n","1-p(x)&=Pr(G=2|X=x)=\\frac{1}{1+\\exp{(\\beta_0+\\beta^Tx)}}.\n","\\end{align}\n","\n","Then, by dividing both expressions and taking the log\n","\n","$$\n","\\log\\frac{Pr(G=1|X=x)}{Pr(G=2|X=x)}=\\log\\frac{p(x)}{1-p(x)}=\\beta_0+\\beta^Tx.\n","$$\n","\n","We note that monotone transformations of liner functions are linear, which ensures that decision boundaries are linear. For the case of one input, increasing $x$ by one unit changes the log-odds by $\\beta_1$ or the odds by $\\exp(\\beta_1)$. For instance, if $9$ out of $10$ default, then the probability of default is $p(x)=0.9$ and the odds are $9$."]},{"cell_type":"markdown","metadata":{"id":"Qlhzz909Z_Cz"},"source":["For more than two classes, the model has the form\n","\n","\\begin{align}\n","\\log\\frac{Pr(G=1|X=x)}{Pr(G=K|X=x)}=&\\beta_{10}+\\beta_1^Tx,\\\\\n","\\log\\frac{Pr(G=2|X=x)}{Pr(G=K|X=x)}=&\\beta_{20}+\\beta_2^Tx,\\\\\n","\\ldots,\\\\\n","\\log\\frac{Pr(G=K-1|X=x)}{Pr(G=K|X=x)}=&\\beta_{(K-1)0}+\\beta_{K-1}^Tx, \n","\\end{align}\n","\n","which models the posterior probabilities of the $K$ classes and ensures to be less than one, where posteriors are given as \n","\n","\\begin{align}\n","Pr(G=k|X=x)=&\\frac{\\exp{(\\beta_{k0}+\\beta_k^Tx)}}{1+\\sum_l\\exp{(\\beta_{l0}+\\beta_l^Tx)}},\\\\\n","Pr(G=K|X=x)=&\\frac{1}{1+\\sum_{l=1}^{K-1}\\exp{(\\beta_{l0}+\\beta_l^Tx)}}.\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"Ebd76DUF6XhP"},"source":["The logistic regression model is usually fit by maximum likelihood. The likelihood for $N$ observations is given by\n","\n","$$\n","l(\\beta)=\\sum_{i=1}^N\\log p_k(x_i; \\beta), \n","$$\n","\n","where $p_k(x_i)=Pr(G=k|X=x_i; \\beta)$. The previous expression can be maximized using Newton-type optimization methods. "]},{"cell_type":"markdown","metadata":{"id":"Yp7DVeZRWcv0"},"source":["Explanatory analysis can be done using hypothesis testing on the estimated coefficients. Given the estimated parameters, we can compute the z-statistic as $z_i=\\hat{\\beta}_i/SE(\\hat{\\beta}_i)$, where a large $z$ holds against the null hypothesis $H_0:\\beta_i=0$.\n","\n","The decision boundary is the set of points for which log-odds are zero, ie. the hyperplane defined by $\\{x|\\beta_0+\\beta^Tx=0\\}$. We remark, that non-linear boundaries can be obtained with logistic regression by adding polynomial terms as inputs. "]},{"cell_type":"markdown","metadata":{"id":"3edx-t31B97C"},"source":["## Discriminant analysis"]},{"cell_type":"markdown","metadata":{"id":"gNr_OiXsCL0d"},"source":["Instead of modelling directly the posterior distribution, discriminant analysis models first the probability density in each class and then applies Bayes' theorem to compute the posterior. Then one classifies to the highest density. Let $f_k(x)=Pr(X=x|G=k)$ the class-conditional density and $\\pi_k=Pr(K=k)$ the prior probability for class $k$, with $\\sum_k=\\pi_k=1$, then the posterior is given by\n","\n","$$\n","Pr(G=k|X=x)=\\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K\\pi_l f_l(x)}.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"_ZTi80Qkmfzj"},"source":["### Linear discriminat analysis"]},{"cell_type":"markdown","metadata":{"id":"ij4gf1MFm5xL"},"source":["Linear discriminant analysis (LDA) models each class as a multivariate normal distribution, with classes sharing the same covariance, ie. $X\\sim\\mathcal{N}(\\mu,\\Sigma)$, $\\Sigma_k=\\Sigma \\quad\\forall k$, and \n","\n","\\begin{align}\n","f_k(x)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}\\exp{(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu))}.\n","\\end{align}\n","\n","Comparing two classes:\n","\n","\\begin{align}\n","\\log\\frac{Pr(G=k|X=x)}{Pr(G=l|X=x)}=\\log\\frac{\\pi_k}{\\pi_l}+\\log\\frac{f_k(x)}{f_l(x)}\n","=\\log\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k+\\mu_l)^T\\Sigma^{-1}(\\mu_k-\\mu_l)+x^T\\Sigma^{-1}(\\mu_k-\\mu_l),\n","\\end{align}\n","\n","which is linear in $x$, as quadratic terms vanished. For the decision boundaries it holds that $Pr(G=k|X=x)=Pr(G=l|X=x)$ and can be obtained as $\\delta_k(x)=\\delta_l(x)$, which correspond to hyperplanes in $\\mathbb{R}^p$. The previous equation shows that logits are also linear as in the case of logistic regression, which could explain why LDA and logistic regression generally lead to similar results. \n","\n","For the case of prior densities being equal, $\\pi_k=\\pi$, decision boundaries between classes pass through the averaged positions of their respective means, $(\\mu_k+\\mu_l)/2$ (see figure).\n"]},{"cell_type":"markdown","metadata":{"id":"TiHCjUghyWPi"},"source":["The linear discriminant functions are obtained from the previous equation\n","\n","$$\n","\\delta_k(x)=\\log\\pi_k-\\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k+x^T\\Sigma^{-1}\\mu_k,\n","$$\n","\n","so $G(x)=\\arg\\max_k\\delta_k(x)$. The parameters of the Gaussian distribution are obtained as\n","\n","$$\n","\\pi_k=\\frac{N_k}{N},\n","\\hat{\\mu}_k=\\sum_{g_i=k}x_i/N_k, \\hat{\\Sigma}=\\sum_k\\sum_{g_i=k}(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T/(N-K). \n","$$\n","\n","We can see that the prior density accounts for the number of samples in the class, $N_k$, and the covariance is based on the distance to the classes centroids and is normalized by the number of dof $N-K$."]},{"cell_type":"markdown","metadata":{"id":"FAQWfXhFETlD"},"source":["Reduced-rank LDA allows to display optimal subspaces of data, for which centroids are spread as much as possible. The K centroids in $p$-dim input space lie within a $(K-1)$-dim subspace. This dimension can be reduced further by selecting few directions of optimal subspaces. Fisher proposed a linear combination of data such that the between-class variance is maximed relative to the within-class variance. These can be obtained by finding the PCs of the centroids."]},{"cell_type":"markdown","metadata":{"id":"8vaTRK4432og"},"source":["### Quadratic discriminant analysis"]},{"cell_type":"markdown","metadata":{"id":"6j2tutgh37Z4"},"source":["Quadratic discriminant analysis (QDA) models a different covariance for each class as $\\Sigma_k$. This leads to quadratic discriminant functions, as quadratic terms are not cancelled out as in LDA,\n","\n","$$\n","\\delta_k(x)=\\log\\pi_k-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)-\\frac{1}{2}\\log|\\Sigma_k|,\n","$$\n","\n","which are obtained directly from the class density. \n","\n","QDA estimates $kp(p+1)/2$ parameters, which leads to a drastic increase in the number of parameter for large $p$ with respect to LDA. Then, QDA may have higher variance than LDA. If the quadratic assumption is correct or the separation boundary is slightly nonlinear, then QDA can lead to lower error rates than LDA by reducing the bias.   "]},{"cell_type":"markdown","metadata":{"id":"HGQUtdjRHihP"},"source":["### Regularized discriminant analysis"]},{"cell_type":"markdown","metadata":{"id":"iHzkK-_-HuDN"},"source":["Regularized discriminant analysis consist of a trade-of between LDA and QDA by shrinking the separate covariances of QDA towards a common covariance as\n","\n","$$\n","\\Sigma_k(\\alpha)=\\alpha\\Sigma_k+(1-\\alpha)\\Sigma, \n","$$\n","\n","where $\\alpha\\in[0,1]$ is a regularization parameter, which can be selected using validation data or cross-validation. "]},{"cell_type":"markdown","metadata":{"id":"MZ2fIwi368x4"},"source":["### Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"6KxBgYkj7Aqy"},"source":["Naive Bayes assumes that features are independent, which translates into diagonal covariance matrices, $\\Sigma_k=\\sigma^2\\mathbb{I}$. The discriminant function becomes \n","\n","$$\n","\\delta_k(x)=\\log(\\pi_kf_k(x))=-\\frac{1}{2}\\left(\\frac{\\|x-\\mu_k\\|^2}{\\sigma_k^2} + \\log\\sigma_k^2\\right)+\\log\\pi_k.\n","$$\n","\n","Naive Bayes can be useful when $p$ is large, in which case other methods can break down. In this case, fewer parameters need to be obtained, which may reduce variance with a small increase in bias. "]},{"cell_type":"markdown","metadata":{"id":"e-NQIalT7B_n"},"source":["## Comparison of methods"]},{"cell_type":"markdown","metadata":{"id":"KEaOz7Hq7HwR"},"source":["No one method will dominate in all cases. For linear problems, LDA and logistic regression will lead to good results. They both have linear boundaries and lead to generally very similar results. LDA assumes a multivariate Gaussian distribution so it can lead to better results if the condition holds. Logistic regression is very popular for $k=2$ but can be unstable for small $N$ or when classes are well separated. LDA is more popular in the opposite cases and when the Gaussian assumption holds. \n","\n","K-NN can lead to better results than the other methods for highly nonlinear decision boundaries. QDA may lead to better results than K-NN for slightly nonlinear problems for low $n$, as QDA makes some assumptions on the decision boundaries and K-NN is a non-parametric method, with no assumptions on the boubdary. QDA is attractive for a low number of features. \n","\n","Naive Bayes can be an option when $p$ is very large. \n","\n","In general, LDA and QDA lead to good results for many problems as they are stable and linear or quadratic boundaries may be sufficient. "]},{"cell_type":"markdown","metadata":{"id":"Fs-Huc_jFRGk"},"source":["## Alternative methods"]},{"cell_type":"markdown","metadata":{"id":"WAqTCHeDFkFu"},"source":["Alternative methods when data are not separable, such as SVM, K-NN, SVM, Decision Trees, Random Forest, Bagging, AdaBoost, will be described in separate chapters. \n"," "]},{"cell_type":"markdown","metadata":{"id":"uxbgp_G2Gsg3"},"source":["# Examples"]},{"cell_type":"markdown","metadata":{"id":"YVfcci6JGudO"},"source":["Link to examples in R and Python Scikit-Learn are given below. \n","\n","*   [Classification in R - Example S&P 500](https://colab.research.google.com/drive/1ji_5NzWL_JmrKGSTy49k-5i4PF0glwWO#scrollTo=8bE8yP1Ohf0B): Predict direction on percentage return of S&P 500 using logistic regression, LDA, QDA and K-NN. \n","* [Classification in R - Example Caravan insurance](https://colab.research.google.com/drive/1EzIgorpz0mj6KkgOfasdNXkL-Rrypf-1#scrollTo=8bE8yP1Ohf0B): Predict customers that buy insurance using logistic regression and K-NN.  \n","* [Classification in Python - Example iris dataset](https://colab.research.google.com/drive/11icBQQRi2R2GsmrUgblNEi0fgVrSjLY8#scrollTo=8bE8yP1Ohf0B): Comparison of wide range ol algorithms: logistic regression, K-NN, SVC, decission trees, random forest and ensembme methods (voting classifier and Adaboost). "]}]}