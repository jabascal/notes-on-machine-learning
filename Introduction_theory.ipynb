{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction_theory.ipynb","provenance":[],"authorship_tag":"ABX9TyP6DYghgi/H4A4fbx6ghOHM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mV7Ze-DFpsbd"},"source":["# Information theory"]},{"cell_type":"markdown","metadata":{"id":"9cZkDYGg-lU_"},"source":["## Self information"]},{"cell_type":"markdown","metadata":{"id":"SeKJrW4pp1jS"},"source":["Information theory allows designing optimal codes and calculating the expected length of messages sampled from a probability distribution. This theory builds up by taking into several premises. Unlikely events are more informative than likely ones and independent events provide additive information. Self information is defined as \n","\n","$$\n","I(x)=-\\log P(x). \n","$$\n","\n","Information can be then quantified, where $1$ *nat* is the information gained by observing an event of probability $1/e$, as $-\\log_e (1/e)=1$. For the base-$2$ logarithm, $1$ *bit* is the information gained by observing an event with probability $1/2$, as $-\\log_{e}1/e=1$. The logarithm ensures that observing two independent events, $A$ and $B$, provide additive information, as $p(A,B)=p(A)p(B)$, and so $I(A, B)=I(A)+I(B)$. Next figure shows that the higher the probability the lower the information.\n"]},{"cell_type":"code","metadata":{"id":"A5x_RMEP3nPV","executionInfo":{"status":"ok","timestamp":1621415171075,"user_tz":-120,"elapsed":538,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"}}},"source":["# Import some dependencies. \n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"ESG-i6ZD4Bld","executionInfo":{"status":"ok","timestamp":1621421249052,"user_tz":-120,"elapsed":549,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"}},"outputId":"1b8fb106-eb7c-4e2a-e2e7-f97451cc37dd"},"source":["# Self information I(p)\n","p=np.arange(0.01,1,0.01)\n","plt.plot(p, -np.log(p))\n","plt.xlabel('P(x)')\n","plt.ylabel('I(P(x))')"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'I(P(x))')"]},"metadata":{"tags":[]},"execution_count":43},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"CoOeip3o-go3"},"source":["## Shannon-entropy"]},{"cell_type":"markdown","metadata":{"id":"_dUbytZA-s0m"},"source":["The Shannon entropy gives an idea of the amount of uncertainty in the entire probability distribution. It is the expected amount of information in an event drawn from a distribution\n","\n","$$\n","H(x)=E_{x\\sim p}[I(x)]=E_{x\\sim p}[-\\log p(x)]=\\int p(x)I(x)dx=-\\int p(x)\\log p(x)dx.\n","$$\n","\n","It can provide a lower bound on the number of bits needed to encode a symbol. Uniform distributions have high entropy, where $H=\\sum_i p_i\\log p_i$. For instance, for a perfect dice, $p_i=p$ so $H=-\\sum_i p_i\\log p_i=-N(1/N)\\log 1/N= \\log N=1.79$; for a dice with $p(6)=0.5$ and $p(x\\neq 6)=0.1$, $H=1.5$; for a dice biased to one number, $p(x_0)=1$, $p(x\\neq x_0)=0$, $H=p_0\\log p_0=0$, as almost deterministic distributions have low entropy. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUlhsHxwG58I","executionInfo":{"status":"ok","timestamp":1621421285646,"user_tz":-120,"elapsed":505,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"}},"outputId":"42c49a6f-6965-44d8-8061-ffbc2f0b2a64"},"source":["# Perfect dice\n","p=1/6; H_p=-6*p*np.log(p); \n","\n","# Dice biased to one number\n","p=(0.1,0.1,0.1,0.1,0.1,0.5); \n","H_np=-sum([p[i]*np.log(p[i]) for i in range(6)])\n","print('Entropy for a perfect dice (H_p=%.2f) is higher than for a non-perfect dice with p(6)=0.3 (H_np=%.2f)' % (H_p, H_np))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Entropy for a perfect dice (H_p=1.79) is higher than for a non-perfect dice with p(6)=0.3 (H_np=1.50)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V_fgec7APZiR"},"source":["## Kullback-Leibler (KL) divergence"]},{"cell_type":"markdown","metadata":{"id":"QPa-rbsAPfuR"},"source":["The KL divergence is used to measure the distance between two distribution. It quantifies the extra information needed to send a message containing symbols drawn from the distribution $P$ using a code that has been designed for the distribution $Q$,\n","\n","$$\n","D_{KL}(P||Q)=E_{x\\sim P}\\left[\\log\\frac{P(x)}{Q(x)}\\right]=\\int p(x)[\\log p(x)-\\log q(x)]dx.\n","$$\n","\n","$D_{KL}(P||Q)\\geq 0$, and $D_{KL}(P||Q)=0$ when $P=Q$, but it is not a true distance as $D_{KL}(P||Q)\\neq D_{KL}(Q||P)$."]},{"cell_type":"markdown","metadata":{"id":"xBUI_18mRZmY"},"source":["## Cross-entropy"]},{"cell_type":"markdown","metadata":{"id":"WxFhhllKRcNQ"},"source":["The cross-entropy measures the expected amount of information needed to encode a message containing symbols drawn from the distribution $P$ using a code that has been designed for the distribution $Q$\n","\n","$$\n","H(P,Q)=-E_{x\\sim P}[\\log Q(x)]=-\\int p(x)\\log q(x)dx.\n","$$\n","\n","From the definition of KL divergence, we see that\n","\n","$$\n","H(P,Q)=H(P)+D_{KL}(P||Q)=-\\int p(x)\\log p(x)-\\int p(x)\\left[\\log\\frac{p(x)}{q(x)}\\right].\n","$$"]},{"cell_type":"markdown","metadata":{"id":"rdwwBHh1pVA5"},"source":["# Bayesian data analysis"]},{"cell_type":"markdown","metadata":{"id":"OfuHqpAJwW3y"},"source":["## Bayes' rule"]},{"cell_type":"markdown","metadata":{"id":"LdPj6UAXpxiq"},"source":["Bayesian inference aims to fit a probability model to a dat set, building a probability distribution on the model parametes and unobserved quantities. For this, Bayesian inference follows the following steps [A Gelman et al, Bayesian data analysis, Chapman&Hall/CRC, 2004]. First, we build the joint probability distribution for all quantities\n","\n","$$\n","p(\\theta,y)=p(\\theta)p(y|\\theta),\n","$$\n","\n","where $p(\\theta)$ is the prior distribution and $p(y|\\theta)$ is the samplig or data distribution.\n","\n","Second, conditioning on the observed data by calculating the posterior distribution, using Bayes' rule\n","\n","$$\n","p(\\theta|y)=\\frac{p(\\theta,y)}{p(y)}=\\frac{p(\\theta)p(y|\\theta)}{p(y)},\n","$$\n","\n","where $p(y)$ can be expressed as the sum over all possible values of $\\theta$\n","\n","$$\n","p(y)=\\int p(\\theta)p(y|\\theta)d\\theta\n","$$\n","\n","Third, evaluating the model. "]},{"cell_type":"markdown","metadata":{"id":"f5Xxejjm-2g8"},"source":["For the case of three or more variables\n","\n","$$\n","p(u,v,w)=\\int p(u,v|w)p(w)=\\int p(u|v,w)p(v|w)p(w).\n","$$"]},{"cell_type":"markdown","metadata":{"id":"n7j6FzJBwnIL"},"source":["## Prediction"]},{"cell_type":"markdown","metadata":{"id":"P-A9ZRdrwpqK"},"source":["Let $y=(y_1,\\ldots,y_n)$ be the observable data (a measurement repeated $n$ times), $\\theta=(\\mu,\\sigma^2)$ its true value and variance, and $\\tilde{y}$ a new unknown observable measurement. Predictive inference allows to make inference about this unknown observable quantity. We model the distribution of the observed variable or *marginal distribution* \n","\n","$$\n","p(y)=\\int p(\\theta,y)d\\theta=\\int p(\\theta)p(y|\\theta)d\\theta,\n","$$\n","\n","also called *prior predictive distribution*, prior as it is not conditional, and predictive as it is an observable. Once data is modelled, we can make inference about an unknown variable $\\tilde{y}$ by calculating the *posterior predictive distribution*, posterior as it is conditional on the observed data and predictive as it is a prediction for an observable\n","\n","$$\n","p(\\tilde{y}|y)=\\int p(\\tilde{y},\\theta|y)d\\theta=\\int p(\\tilde{y}|\\theta,y)p(\\theta|y)d\\theta=\\int p(\\tilde{y}|\\theta)p(\\theta|y)d\\theta, \n","$$\n","\n","where in the last equation we use that $y$ and $\\tilde{y}$ are conditionally independent given $\\theta$."]},{"cell_type":"markdown","metadata":{"id":"HiwU_zZqQaxG"},"source":["## Expectation"]},{"cell_type":"markdown","metadata":{"id":"HPZ4CbZNQijF"},"source":["The expected value of a random variable is given by\n","\n","$$\n","E_x[x]=\\int x p(x)dx\n","$$. \n","\n","The conditional expectation of a random variable is the expected value integrating over its conditional distribution\n","\n","$$\n","E_x[x|y]=\\int x p(x|y)dx. \n","$$\n","\n","Inserting the marginal distribution $p(x)=\\int p(x,y)dy$ into the expected value\n","$$\n","E_x[x]=\\int_x x p(x)dx=\\int_x\\int_y x p(x,y)dydx=\\int_y\\int_x x p(x|y)p(y)dxdy=\\int_y E_x[x|y]p(y)dy=E_y[E_x[x|y]].\n","$$.\n"]},{"cell_type":"markdown","metadata":{"id":"SoEMxthOYEvP"},"source":["$$\n","E_{(x,y)}[f]=\n","$$"]},{"cell_type":"markdown","metadata":{"id":"04DDoS1SjgxR"},"source":["# Machine learning and statistical learning : Definitions"]},{"cell_type":"markdown","metadata":{"id":"cuf4vh-TD0ZM"},"source":["## Supervised, unsupervised and reinforcement learning"]},{"cell_type":"markdown","metadata":{"id":"BbU1bciejoLP"},"source":["By *learning* we mean a computer program that learns from experience given a task and a performance metric and improves its performance with training. Machine learning and statistical learning share same methods, but while the former was developed in computer science, the latter was born in statistics with emphasis in statistical explanation (with an understanding of the inputs variables and statistical assessment of predictions). \n","\n","*Unsupervised learning* (UL) aims to learn properties and structure of data. In most cases, it can be understood as learning the probability distribution of data $p(X)$ by observing $X$, as in density estimation or denoising. Another case is clustering. There are not output variables or gold standard, only inputs, also called features or predictors. Data or space are grouped by similarity of features and can provide information about data structure or organization. \n","\n","*Supervised learning* (SL) can be seen as learning the posterior distribution $p(X|Y)$ by observing both input variables $X$ and target or output variable $Y$. This accounts for regression problems, which account for quantitative variables, and classification, where outputs constitute a finite qualitative set. Data need to be labelled, it requires input pairs $(x_i,y_i)$. UL is also useful for data processing before SL or for simplification, as labelling data is expensive. \n","\n","UL can be also seen as solving $n$ supervised problems\n","$$\n","p(X)=\\Pi_i p(X_i|X_1,\\ldots,X_{i-1}), \n","$$\n","and a SL problem using UL to learn $p(X,Y)$\n","$$\n","p(X|Y)=\\frac{p(X,Y)}{\\sum_x p(X,Y)}.\n","$$\n","\n","*Reinforcement learning* interact with the enviroment, so there is constant input an not a fixed data set."]},{"cell_type":"markdown","metadata":{"id":"TdMLrc1qEybM"},"source":["## Statistical decision theory"]},{"cell_type":"markdown","metadata":{"id":"VaGP6YVTD4Lz"},"source":["Let $X$ be a vector on inputs, $Y$ the target variable, $f(X)$ the prediction model and $L(Y,f(X))$ the loss function for penalizing errors in the prediction. The *test error*, also named *generalization error*, is the *expected prediction error* (EPE) over the test set\n","\n","$$\n","EPEErr=E[L(Y,f(X))]=\\int L(Y,f(X))P(dx,dy),\n","$$\n","\n","where $X$ and $Y$ are randomly sample from their joint distribution $P(X,Y)$. \n","Training error is the average loss over the training set\n","\n","$$\n","Err_{train}=\\sum_{i=1}^N[L(y_i,f(x_i))]\n","$$\n","\n","For regression problems, variables are quantitative and typical loss is the squared error\n","\n","$$\n","L(X)=(Y-f(X))^2,\n","$$ \n","\n","or the absolute error, $L(X)=|Y-f(X)|$. \n","\n","For qualitative variables, the output variable or response $G$ takes values $1, 2, \\ldots, K$. Typical loss is the $0$-$1$ loss, $L(G,\\hat{G}(X))=I(G\\neq\\hat{G}(X))$, or the log-likelihood, also named *cross-entropy loss* or *deviance*\n","\n","$$\n","L(G,\\hat{G}(X))=-2\\sum_{k=1}^KI(G=k)\\log p_k(X)=-2\\log p_G(X). \n","$$"]},{"cell_type":"markdown","metadata":{"id":"DHm7XBaaIdW8"},"source":["The EPE for squarred error loss\n","\n","$$\n","EPE(f)=E_{(x,y)}[L(Y,f(X))]=\\int L(y,f(x)) p(dx,dy)=\\int_x\\int_y L(y,f(x)) p(x,y)dxdy=\\int_x\\int_y L(y,f(x)) p(y|x)p(x)dxdy=E[L(y,f(x))]\n","$$"]}]}