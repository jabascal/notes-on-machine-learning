{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_assessment.ipynb","provenance":[],"authorship_tag":"ABX9TyPOKssu0ZtxJlnQqXQI0OC4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hEA06EZ9eVEF"},"source":["# Model assessment"]},{"cell_type":"markdown","metadata":{"id":"TmciQBbH92Ak"},"source":["## Theory"]},{"cell_type":"markdown","metadata":{"id":"amMPPuHk-Sjj"},"source":["Model assessment aims at quantifying the model generalization performance by estimating the test error. While there are analytical methods, such as the Bayesian information criterion (BIC) and the Akaike information criterion (AIC), numerical methods are widely used. \n","\n","In cases where enough data are available, data are divided into three parts: a training, a validation and a test set. Training set is used to fit the model, validation set is used to estimate the prediction error for model selection and parameter tuning, and test set is used for a final estimation of the generalization error. \n","\n","In cases where data are scarce, resampling methods are used.  Cross-validation (CV) direclty estimates the prediction error and boostrap estimates statistical accuracy. "]},{"cell_type":"markdown","metadata":{"id":"iRZWfG0slpXs"},"source":["## Generalization error"]},{"cell_type":"markdown","metadata":{"id":"VaGP6YVTD4Lz"},"source":["Let $X$ be a vector on inputs, $Y$ the target variable, $f(X)$ the prediction model and $L(Y,f(X))$ the loss function. The *test error*, also named *generalization error*, is the expected error over the test set\n","$$\n","Err=E(Y,f(X)),\n","$$\n","where $X$ and $Y$ are randomly sample from their joint distribution. \n","Training error is the average loss over the training set\n","$$\n","Err_{train}=\\sum_{i=1}^N[L(y_i,f(x_i))]\n","$$\n","\n","For regression problems, variables are quantitative and a typical loss is the squared error\n","$$\n","L(X)=(Y-f(X))^2,\n","$$ \n","or the absolute error, $L(X)=|Y-f(X)|$. \n","\n","For qualitative variables, the output variable or response $G$ takes values $1, 2, \\ldots, K$. Typical loss is the $0$-$1$ loss, $L(G,\\hat{G}(X))=I(G\\neq\\hat{G}(X))$, or the log-likelihood, also named *cross-entropy loss* or *deviance*\n","$$\n","L(G,\\hat{G}(X))=-2\\sum_{k=1}^KI(G=k)\\log p_k(X)=-2\\log p_G(X). \n","$$"]}]}